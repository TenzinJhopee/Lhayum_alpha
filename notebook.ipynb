{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7423f2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MM\\OneDrive\\Desktop\\HackthonBern\\Bhopa_alpha\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ÈóÆÈ¢ò', 'Á≠îÊ°à', 'ÊñáÊú¨', 'Unnamed: 3', 'Ê†áÈ¢ò'], dtype='object') (2007, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the TibetanQA dataset\n",
    "df = pd.read_excel('TibetanQA.xlsx')\n",
    "\n",
    "# Inspect data structure\n",
    "print(df.columns, df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f1631f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['question', 'answer', 'text', 'Unnamed: 3', 'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "978c055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and prepare data\n",
    "# Assuming columns: 'article', 'question', 'answer'\n",
    "df = df.dropna(subset=['question', 'text'])\n",
    "df['qa_pair'] = df['question'] + ' ' + df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50e35587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "\n",
    "# Test with Tibetan text\n",
    "test_tibetan = \"‡Ω†‡Ωë‡Ω≤‡ºã‡Ωì‡Ω≤‡ºã‡Ωñ‡Ωº‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡ΩÇ‡Ω≤‡ºã‡Ωë‡Ωî‡Ω∫‡ºã‡Ωû‡Ω≤‡ΩÇ‡ºã‡Ω°‡Ω≤‡Ωì‡ºç\"\n",
    "test_embedding = model.encode(test_tibetan)\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25f7f429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [02:23<00:00,  6.84s/it]\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize ChromaDB\n",
    "client = chromadb.PersistentClient(path=\"./tibetan_qa_db\")\n",
    "collection = client.create_collection(\"tibetan_qa\")\n",
    "\n",
    "# Embed and store Q&A pairs\n",
    "questions = df['question'].tolist()\n",
    "answers = df['text'].tolist()\n",
    "qa_pairs = df['qa_pair'].tolist()\n",
    "\n",
    "# Generate embeddings (batch processing)\n",
    "batch_size = 100\n",
    "for i in tqdm(range(0, len(questions), batch_size)):\n",
    "    batch_questions = questions[i:i+batch_size]\n",
    "    batch_answers = answers[i:i+batch_size]\n",
    "    batch_qa = qa_pairs[i:i+batch_size]\n",
    "    \n",
    "    embeddings = model.encode(batch_questions)\n",
    "    \n",
    "    collection.add(\n",
    "        embeddings=embeddings.tolist(),\n",
    "        documents=batch_qa,\n",
    "        metadatas=[{\"question\": q, \"answer\": a} for q, a in zip(batch_questions, batch_answers)],\n",
    "        ids=[f\"qa_{j}\" for j in range(i, i+len(batch_questions))]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "787dcf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "class ApertusSwissLLM:\n",
    "    def __init__(self, api_key=None, base_url=\"https://chat.publicai.co\"):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        \n",
    "    def generate_response(self, prompt, context=\"\", max_tokens=500):\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": \"swiss-llm\",  # Adjust model name as needed\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": f\"Context: {context}\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "        \n",
    "        response = requests.post(f\"{self.base_url}/chat/completions\", \n",
    "                               headers=headers, json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()['choices'][0]['message']['content']\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\"\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ApertusSwissLLM(api_key=\"sk-768c82ef24604a4db381bf8588a73007\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d9540cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TibetanRAGSystem:\n",
    "    def __init__(self, collection, embedding_model, llm):\n",
    "        self.collection = collection\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm = llm\n",
    "        \n",
    "    def retrieve_relevant_qa(self, query, n_results=5):\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        results = self.collection.query(\n",
    "            query_embeddings=query_embedding.tolist(),\n",
    "            n_results=n_results\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_answer(self, query):\n",
    "        # Retrieve relevant Q&A pairs\n",
    "        relevant_qa = self.retrieve_relevant_qa(query)\n",
    "        \n",
    "        # Prepare context from retrieved documents\n",
    "        context = \"\"\n",
    "        for i, doc in enumerate(relevant_qa['documents'][0]):\n",
    "            metadata = relevant_qa['metadatas'][0][i]\n",
    "            context += f\"Q: {metadata['question']}\\nA: {metadata['answer']}\\n\\n\"\n",
    "        \n",
    "        # Generate system prompt for Tibetan\n",
    "        system_prompt = f\"\"\"\n",
    "        You are a helpful assistant that answers questions in Tibetan based on the provided context.\n",
    "        Use the following question-answer pairs as reference to answer the user's question.\n",
    "        If you cannot find relevant information in the context, politely say so in Tibetan.\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.llm.generate_response(query, context=system_prompt)\n",
    "        return response, relevant_qa\n",
    "\n",
    "# Initialize RAG system\n",
    "rag_system = TibetanRAGSystem(collection, model, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "028ebe21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tibetan_rag_streamlit.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tibetan_rag_streamlit.py\n",
    "import streamlit as st\n",
    "\n",
    "def main():\n",
    "    st.title(\"‡Ωñ‡Ωº‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã RAG ‡Ωñ‡Ω§‡Ωë‡ºã‡Ωî‡ºã‡Ωî‡Ωº‡ºç (Tibetan RAG Chatbot)\")\n",
    "    st.write(\"Ask questions in Tibetan and get answers from the TibetanQA dataset!\")\n",
    "    \n",
    "    # Initialize session state\n",
    "    if 'messages' not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "    \n",
    "    # Display chat history\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "    \n",
    "    # Chat input\n",
    "    if prompt := st.chat_input(\"‡Ω†‡Ωë‡Ω≤‡Ω¢‡ºã‡Ωë‡æ≤‡Ω≤‡ºã‡Ωñ‡ºã‡Ωñ‡æ≤‡Ω≤‡Ω¶‡ºã‡Ω¢‡Ωº‡ΩÇ‡Ω¶‡ºç (Please write your question here)\"):\n",
    "        # Add user message to chat history\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.markdown(prompt)\n",
    "        \n",
    "        # Generate response\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            with st.spinner(\"Thinking...\"):\n",
    "                response, sources = rag_system.generate_answer(prompt)\n",
    "                st.markdown(response)\n",
    "                \n",
    "                # Show sources\n",
    "                with st.expander(\"Sources\"):\n",
    "                    for i, doc in enumerate(sources['documents'][0]):\n",
    "                        metadata = sources['metadatas'][0][i]\n",
    "                        st.write(f\"**Q:** {metadata['question']}\")\n",
    "                        st.write(f\"**A:** {metadata['answer']}\")\n",
    "                        st.write(\"---\")\n",
    "        \n",
    "        # Add assistant response to chat history\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7adc4cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "App running at: http://localhost:8502\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Cell 2: Run it\n",
    "import os\n",
    "os.system(\"streamlit run tibetan_rag_app.py --server.port 8502 &\")\n",
    "print(\"App running at: http://localhost:8502\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7edbdbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening ports:\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "result = subprocess.run([\"netstat\", \"-an\"], capture_output=True, text=True)\n",
    "print(\"Listening ports:\")\n",
    "for line in result.stdout.split('\\n'):\n",
    "    if ':850' in line and 'LISTEN' in line:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09c6a76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', 'app.py']>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.Popen([\"streamlit\", \"run\", \"app.py\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1407bb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting comm>=0.1.3 (from ipywidgets)\n",
      "  Downloading comm-0.2.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\python\\python38\\lib\\site-packages (from ipywidgets) (8.11.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\python\\python38\\lib\\site-packages (from ipywidgets) (5.9.0)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: backcall in c:\\python\\python38\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\python\\python38\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\python\\python38\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\python\\python38\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\python\\python38\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\python\\python38\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.38)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\python\\python38\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.14.0)\n",
      "Requirement already satisfied: stack-data in c:\\python\\python38\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: colorama in c:\\python\\python38\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\python\\python38\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\python\\python38\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\python\\python38\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\python\\python38\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\python\\python38\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\python\\python38\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Downloading comm-0.2.3-py3-none-any.whl (7.3 kB)\n",
      "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.2 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.2 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 2.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.2 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 1.8 MB/s eta 0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, comm, ipywidgets\n",
      "  Attempting uninstall: comm\n",
      "    Found existing installation: comm 0.1.2\n",
      "    Uninstalling comm-0.1.2:\n",
      "      Successfully uninstalled comm-0.1.2\n",
      "Successfully installed comm-0.2.3 ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\Python\\Python38\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb799522",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipywidgets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mipywidgets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwidgets\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display, HTML, clear_output\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mTibetanRAGNotebook\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ipywidgets'"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "class TibetanRAGNotebook:\n",
    "    def __init__(self, rag_system):\n",
    "        self.rag_system = rag_system\n",
    "        self.messages = []\n",
    "        \n",
    "    def create_interface(self):\n",
    "        # Title\n",
    "        display(HTML(\"<h1>‡Ωñ‡Ωº‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã RAG ‡Ωñ‡Ω§‡Ωë‡ºã‡Ωî‡ºã‡Ωî‡Ωº‡ºç (Tibetan RAG Chatbot)</h1>\"))\n",
    "        display(HTML(\"<p>Ask questions in Tibetan and get answers from the TibetanQA dataset!</p>\"))\n",
    "        \n",
    "        # Chat input\n",
    "        self.text_input = widgets.Text(\n",
    "            placeholder=\"‡Ω†‡Ωë‡Ω≤‡Ω¢‡ºã‡Ωë‡æ≤‡Ω≤‡ºã‡Ωñ‡ºã‡Ωñ‡æ≤‡Ω≤‡Ω¶‡ºã‡Ω¢‡Ωº‡ΩÇ‡Ω¶‡ºç (Please write your question here)\",\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='70%')\n",
    "        )\n",
    "        \n",
    "        self.send_button = widgets.Button(\n",
    "            description=\"Send\",\n",
    "            button_style='primary'\n",
    "        )\n",
    "        \n",
    "        self.output = widgets.Output()\n",
    "        \n",
    "        # Event handler\n",
    "        def on_send_click(b):\n",
    "            query = self.text_input.value\n",
    "            if query.strip():\n",
    "                self.process_query(query)\n",
    "                self.text_input.value = \"\"\n",
    "        \n",
    "        self.send_button.on_click(on_send_click)\n",
    "        \n",
    "        # Display interface\n",
    "        display(widgets.HBox([self.text_input, self.send_button]))\n",
    "        display(self.output)\n",
    "    \n",
    "    def process_query(self, query):\n",
    "        with self.output:\n",
    "            # Show user message\n",
    "            print(f\"üßë You: {query}\")\n",
    "            print(\"ü§ñ Assistant: Thinking...\")\n",
    "            \n",
    "            try:\n",
    "                # Generate response\n",
    "                response, sources = self.rag_system.generate_answer(query)\n",
    "                \n",
    "                # Clear \"thinking\" and show response\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "                # Show conversation\n",
    "                for msg in self.messages:\n",
    "                    print(f\"üßë You: {msg['user']}\")\n",
    "                    print(f\"ü§ñ Assistant: {msg['assistant']}\")\n",
    "                    print(\"-\" * 50)\n",
    "                \n",
    "                print(f\"üßë You: {query}\")\n",
    "                print(f\"ü§ñ Assistant: {response}\")\n",
    "                \n",
    "                # Show sources\n",
    "                print(\"\\nüìö Sources:\")\n",
    "                for i, doc in enumerate(sources['documents'][0][:3]):  # Show top 3\n",
    "                    metadata = sources['metadatas'][0][i]\n",
    "                    print(f\"Q: {metadata['question']}\")\n",
    "                    print(f\"A: {metadata['answer']}\")\n",
    "                    print(\"-\" * 30)\n",
    "                \n",
    "                # Store message\n",
    "                self.messages.append({\"user\": query, \"assistant\": response})\n",
    "                \n",
    "            except Exception as e:\n",
    "                clear_output(wait=True)\n",
    "                for msg in self.messages:\n",
    "                    print(f\"üßë You: {msg['user']}\")\n",
    "                    print(f\"ü§ñ Assistant: {msg['assistant']}\")\n",
    "                    print(\"-\" * 50)\n",
    "                print(f\"üßë You: {query}\")\n",
    "                print(f\"ü§ñ Assistant: Error - {str(e)}\")\n",
    "\n",
    "# Usage in notebook\n",
    "notebook_ui = TibetanRAGNotebook(rag_system)\n",
    "notebook_ui.create_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269b8d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "data did not match any variant of untagged enum ModelWrapper at line 1217953 column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Initialize LLM\u001b[39;00m\n\u001b[0;32m     31\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswiss-ai/Apertus-8B-Instruct-2509\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 32\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mApertusSwissLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mswiss-ai/Apertus-8B-Instruct-2509\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m, in \u001b[0;36mApertusSwissLLM.__init__\u001b[1;34m(self, model_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./apertus-swiss-model\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# load the tokenizer and the model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     10\u001b[0m         model_path,\n\u001b[0;32m     11\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\MM\\OneDrive\\Desktop\\HackthonBern\\Bhopa_alpha\\.venv\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:862\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    860\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    861\u001b[0m         )\n\u001b[1;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[1;32mc:\\Users\\MM\\OneDrive\\Desktop\\HackthonBern\\Bhopa_alpha\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2089\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2086\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2087\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2089\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[0;32m   2090\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   2091\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   2092\u001b[0m     init_configuration,\n\u001b[0;32m   2093\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[0;32m   2094\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2095\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2096\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2097\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   2098\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[0;32m   2099\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2100\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2101\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\MM\\OneDrive\\Desktop\\HackthonBern\\Bhopa_alpha\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2311\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2309\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2311\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[0;32m   2312\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   2313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   2314\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2315\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2316\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\MM\\OneDrive\\Desktop\\HackthonBern\\Bhopa_alpha\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:111\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(tokenizer_object)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fast_tokenizer_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_slow:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_tokenizer_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001b[1;31mException\u001b[0m: data did not match any variant of untagged enum ModelWrapper at line 1217953 column 3"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class ApertusSwissLLM:\n",
    "    def __init__(self, model_path=\"./apertus-swiss-model\"):\n",
    "\n",
    "        # load the tokenizer and the model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "        )\n",
    "        \n",
    "    def generate_response(self, prompt, context=\"\", max_tokens=500):\n",
    "        full_prompt = f\"Context: {context}\\n\\nUser: {prompt}\\nAssistant:\"\n",
    "        \n",
    "        inputs = self.tokenizer.encode(full_prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "        return response.strip()\n",
    "\n",
    "# Initialize LLM\n",
    "model_name = \"swiss-ai/Apertus-8B-Instruct-2509\"\n",
    "llm = ApertusSwissLLM(model_path=\"swiss-ai/Apertus-8B-Instruct-2509\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9886b4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MM\\OneDrive\\Desktop\\HackthonBern\\Bhopa_alpha\\tibetan_qa_db\n"
     ]
    }
   ],
   "source": [
    "from chromadb import PersistentClient\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n",
    "DB_DIR = (BASE_DIR / 'tibetan_qa_db').resolve()\n",
    "\n",
    "print(DB_DIR)\n",
    "client = PersistentClient(path=str(DB_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa923f16",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "data did not match any variant of untagged enum ModelWrapper at line 1217953 column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# for GPU usage or \"cpu\" for CPU usage\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# load the tokenizer and the model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      8\u001b[0m     model_name,\n\u001b[0;32m      9\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\MM\\OneDrive\\Desktop\\HackthonBern\\Bhopa_alpha\\.venv\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:862\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    860\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    861\u001b[0m         )\n\u001b[1;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[1;32mc:\\Users\\MM\\OneDrive\\Desktop\\HackthonBern\\Bhopa_alpha\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2089\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2086\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2087\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2089\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[0;32m   2090\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   2091\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   2092\u001b[0m     init_configuration,\n\u001b[0;32m   2093\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[0;32m   2094\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2095\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2096\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2097\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   2098\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[0;32m   2099\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2100\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2101\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\MM\\OneDrive\\Desktop\\HackthonBern\\Bhopa_alpha\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2311\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2309\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2311\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[0;32m   2312\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   2313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   2314\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2315\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2316\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\MM\\OneDrive\\Desktop\\HackthonBern\\Bhopa_alpha\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:111\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(tokenizer_object)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fast_tokenizer_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_slow:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_tokenizer_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001b[1;31mException\u001b[0m: data did not match any variant of untagged enum ModelWrapper at line 1217953 column 3"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"swiss-ai/Apertus-8B-Instruct-2509\"\n",
    "device = \"cuda\"  # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    ").to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bhopa-alpha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
